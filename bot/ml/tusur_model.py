import numpy as np
import pandas as pd
from sklearn.preprocessing import MultiLabelBinarizer, LabelEncoder
from sklearn.model_selection import train_test_split
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.optimizers import Adam
import pymorphy3 as pymorphy2
import os
import pickle
import json
from pathlib import Path

morph = pymorphy2.MorphAnalyzer()

TUSUR_FACULTIES = {
    "–†–¢–§": {
        "name": "–†–∞–¥–∏–æ—Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–π —Ñ–∞–∫—É–ª—å—Ç–µ—Ç",
        "liked_subjects": ["—Ñ–∏–∑–∏–∫–∞", "–º–∞—Ç–µ–º–∞—Ç–∏–∫–∞", "–∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–∫–∞", "–∞–ª–≥–µ–±—Ä–∞", "–≥–µ–æ–º–µ—Ç—Ä–∏—è"],
        "disliked_subjects": ["–ª–∏—Ç–µ—Ä–∞—Ç—É—Ä–∞", "–±–∏–æ–ª–æ–≥–∏—è", "–∏—Å—Ç–æ—Ä–∏—è", "–æ–±—â–µ—Å—Ç–≤–æ–∑–Ω–∞–Ω–∏–µ", "–º—Ö–∫"],
        "keywords": ["—Ä–∞–¥–∏–æ—Ç–µ—Ö–Ω–∏–∫–∞", "—ç–ª–µ–∫—Ç—Ä–æ–Ω–∏–∫–∞", "—Å–∏–≥–Ω–∞–ª—ã", "–∞–Ω—Ç–µ–Ω–Ω—ã", "—Ç–µ–ª–µ–∫–æ–º–º—É–Ω–∏–∫–∞—Ü–∏–∏", "—Å—Ö–µ–º–æ—Ç–µ—Ö–Ω–∏–∫–∞", "—Ä–∞–¥–∏–æ", "—Å–≤—è–∑—å", "—á–∞—Å—Ç–æ—Ç—ã", "–≤–æ–ª–Ω—ã"]
    },
    "–§–≠–¢": {
        "name": "–§–∞–∫—É–ª—å—Ç–µ—Ç —ç–ª–µ–∫—Ç—Ä–æ–Ω–Ω–æ–π —Ç–µ—Ö–Ω–∏–∫–∏", 
        "liked_subjects": ["—Ñ–∏–∑–∏–∫–∞", "–º–∞—Ç–µ–º–∞—Ç–∏–∫–∞", "–∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–∫–∞", "—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—è", "–∞–ª–≥–µ–±—Ä–∞"],
        "disliked_subjects": ["–ª–∏—Ç–µ—Ä–∞—Ç—É—Ä–∞", "–∏—Å—Ç–æ—Ä–∏—è", "–æ–±—â–µ—Å—Ç–≤–æ–∑–Ω–∞–Ω–∏–µ", "–º—Ö–∫", "–º—É–∑—ã–∫–∞"],
        "keywords": ["—ç–ª–µ–∫—Ç—Ä–æ–Ω–∏–∫–∞", "–º–∏–∫—Ä–æ—Å—Ö–µ–º—ã", "–∞–≤—Ç–æ–º–∞—Ç–∏–∫–∞", "—Ä–æ–±–æ—Ç–æ—Ç–µ—Ö–Ω–∏–∫–∞", "–º–∏–∫—Ä–æ–∫–æ–Ω—Ç—Ä–æ–ª–ª–µ—Ä—ã", "–ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ", "—Å—Ö–µ–º—ã", "–ø—Ä–æ—Ü–µ—Å—Å–æ—Ä—ã", "–¥–∞—Ç—á–∏–∫–∏", "—É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞"]
    },
    "–§–°–£": {
        "name": "–§–∞–∫—É–ª—å—Ç–µ—Ç —Å–∏—Å—Ç–µ–º —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è",
        "liked_subjects": ["–º–∞—Ç–µ–º–∞—Ç–∏–∫–∞", "–∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–∫–∞", "—Ñ–∏–∑–∏–∫–∞", "–æ–±—â–µ—Å—Ç–≤–æ–∑–Ω–∞–Ω–∏–µ", "—ç–∫–æ–Ω–æ–º–∏–∫–∞"],
        "disliked_subjects": ["–±–∏–æ–ª–æ–≥–∏—è", "—Ö–∏–º–∏—è", "–ª–∏—Ç–µ—Ä–∞—Ç—É—Ä–∞", "–º—Ö–∫", "–∏–∑–æ"],
        "keywords": ["—É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ", "—Å–∏—Å—Ç–µ–º—ã", "–∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏—è", "–º–µ–Ω–µ–¥–∂–º–µ–Ω—Ç", "–±–∏–∑–Ω–µ—Å", "–ø—Ä–æ—Ü–µ—Å—Å—ã", "–æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è", "–∞–Ω–∞–ª–∏—Ç–∏–∫–∞", "–ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ", "–∫–æ–Ω—Ç—Ä–æ–ª—å"]
    },
    "–ì–§": {
        "name": "–ì—É–º–∞–Ω–∏—Ç–∞—Ä–Ω—ã–π —Ñ–∞–∫—É–ª—å—Ç–µ—Ç",
        "liked_subjects": ["–ª–∏—Ç–µ—Ä–∞—Ç—É—Ä–∞", "–∏—Å—Ç–æ—Ä–∏—è", "–æ–±—â–µ—Å—Ç–≤–æ–∑–Ω–∞–Ω–∏–µ", "—Ä—É—Å—Å–∫–∏–π_—è–∑—ã–∫", "–∞–Ω–≥–ª–∏–π—Å–∫–∏–π", "–º—Ö–∫"],
        "disliked_subjects": ["—Ñ–∏–∑–∏–∫–∞", "–º–∞—Ç–µ–º–∞—Ç–∏–∫–∞", "—Ö–∏–º–∏—è", "–∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–∫–∞", "–∞–ª–≥–µ–±—Ä–∞"],
        "keywords": ["–ø—Å–∏—Ö–æ–ª–æ–≥–∏—è", "—Å–æ—Ü–∏–æ–ª–æ–≥–∏—è", "–ª–∏–Ω–≥–≤–∏—Å—Ç–∏–∫–∞", "–∫—É–ª—å—Ç—É—Ä–∞", "–æ–±—â–µ—Å—Ç–≤–æ", "–∫–æ–º–º—É–Ω–∏–∫–∞—Ü–∏–∏", "—è–∑—ã–∫–∏", "—Ñ–∏–ª–æ—Å–æ—Ñ–∏—è", "–∏—Å–∫—É—Å—Å—Ç–≤–æ", "–≥—É–º–∞–Ω–∏—Ç–∞—Ä–Ω—ã–µ"]
    },
    "–§–ò–¢": {
        "name": "–§–∞–∫—É–ª—å—Ç–µ—Ç –∏–Ω–Ω–æ–≤–∞—Ü–∏–æ–Ω–Ω—ã—Ö —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–π",
        "liked_subjects": ["–∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–∫–∞", "–º–∞—Ç–µ–º–∞—Ç–∏–∫–∞", "—Ñ–∏–∑–∏–∫–∞", "–∞–ª–≥–µ–±—Ä–∞", "–≥–µ–æ–º–µ—Ç—Ä–∏—è"],
        "disliked_subjects": ["–±–∏–æ–ª–æ–≥–∏—è", "—Ö–∏–º–∏—è", "–ª–∏—Ç–µ—Ä–∞—Ç—É—Ä–∞", "–∏—Å—Ç–æ—Ä–∏—è", "–º—Ö–∫"],
        "keywords": ["–ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ", "–∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç", "–∞–ª–≥–æ—Ä–∏—Ç–º—ã", "—Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∞", "it", "–∏–Ω–Ω–æ–≤–∞—Ü–∏–∏", "—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏", "—Å–æ—Ñ—Ç", "–¥–∞–Ω–Ω—ã–µ", "—Ü–∏—Ñ—Ä–æ–≤—ã–µ"]
    }
}

SCHOOL_SUBJECTS_LIST = [
    "–º–∞—Ç–µ–º–∞—Ç–∏–∫–∞", "—Ä—É—Å—Å–∫–∏–π_—è–∑—ã–∫", "–ª–∏—Ç–µ—Ä–∞—Ç—É—Ä–∞", "—Ñ–∏–∑–∏–∫–∞", "—Ö–∏–º–∏—è", 
    "–±–∏–æ–ª–æ–≥–∏—è", "–≥–µ–æ–≥—Ä–∞—Ñ–∏—è", "–∏—Å—Ç–æ—Ä–∏—è", "–æ–±—â–µ—Å—Ç–≤–æ–∑–Ω–∞–Ω–∏–µ", "–∞–Ω–≥–ª–∏–π—Å–∫–∏–π",
    "–Ω–µ–º–µ—Ü–∫–∏–π", "—Ñ—Ä–∞–Ω—Ü—É–∑—Å–∫–∏–π", "–∫–∏—Ç–∞–π—Å–∫–∏–π", "–∏—Å–ø–∞–Ω—Å–∫–∏–π", "–∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–∫–∞", 
    "—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—è", "–∞–ª–≥–µ–±—Ä–∞", "–≥–µ–æ–º–µ—Ç—Ä–∏—è", "–∞—Å—Ç—Ä–æ–Ω–æ–º–∏—è", "—ç–∫–æ–ª–æ–≥–∏—è", 
    "–ø—Ä–∞–≤–æ", "—ç–∫–æ–Ω–æ–º–∏–∫–∞", "–º—Ö–∫", "–∏–∑–æ", "–º—É–∑—ã–∫–∞", "—á–µ—Ä—á–µ–Ω–∏–µ", 
    "—Ñ–∏–∑–∫—É–ª—å—Ç—É—Ä–∞", "–æ–±–∂"
]

class TusurFacultyPredictor:
    def __init__(self):
        self.mlb = MultiLabelBinarizer(classes=SCHOOL_SUBJECTS_LIST)
        self.label_encoder = LabelEncoder()
        self.model = None
        self.all_keywords = []
        self.is_trained = False
        self.model_path = Path("bot/ml/trained_model")
        self.model_path.mkdir(parents=True, exist_ok=True)
        
    def lemmatize_text(self, text):
        if not text:
            return ""
        
        words = text.lower().split()
        lemmatized_words = []
        
        for word in words:
            clean_word = ''.join(char for char in word if char.isalpha())
            if clean_word:
                try:
                    parsed = morph.parse(clean_word)[0]
                    lemmatized_words.append(parsed.normal_form)
                except:
                    lemmatized_words.append(clean_word)
        
        return ' '.join(lemmatized_words)
    
    def generate_training_data(self, num_samples=2000):
        data = []
        faculty_codes = list(TUSUR_FACULTIES.keys())

        self.all_keywords = []
        for faculty_info in TUSUR_FACULTIES.values():
            self.all_keywords.extend(faculty_info["keywords"])
        self.all_keywords = list(set(self.all_keywords))
        
        print(f"–ì–µ–Ω–µ—Ä–∞—Ü–∏—è {num_samples} –æ–±—Ä–∞–∑—Ü–æ–≤ –¥–ª—è {len(faculty_codes)} —Ñ–∞–∫—É–ª—å—Ç–µ—Ç–æ–≤...")
        
        for i in range(num_samples):
            if i % 400 == 0:
                print(f"–°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ {i}/{num_samples} –æ–±—Ä–∞–∑—Ü–æ–≤")
                
            faculty_code = np.random.choice(faculty_codes)
            faculty = TUSUR_FACULTIES[faculty_code]

            liked_available = faculty["liked_subjects"].copy()
            liked_count = np.random.randint(2, min(6, len(liked_available) + 1))
            liked = np.random.choice(liked_available, size=min(liked_count, len(liked_available)), replace=False).tolist()

            if np.random.random() < 0.3:
                other_subjects = [s for s in SCHOOL_SUBJECTS_LIST if s not in liked and s not in faculty["disliked_subjects"]]
                if other_subjects:
                    liked.append(np.random.choice(other_subjects))

            disliked_available = faculty["disliked_subjects"].copy()
            disliked_count = np.random.randint(1, min(5, len(disliked_available) + 1))
            disliked = np.random.choice(disliked_available, size=min(disliked_count, len(disliked_available)), replace=False).tolist()

            exams = liked.copy()
            if np.random.random() < 0.4:
                additional_exams = ["—Ä—É—Å—Å–∫–∏–π_—è–∑—ã–∫", "–º–∞—Ç–µ–º–∞—Ç–∏–∫–∞", "—Ñ–∏–∑–∏–∫–∞", "–∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–∫–∞", "–æ–±—â–µ—Å—Ç–≤–æ–∑–Ω–∞–Ω–∏–µ"]
                additional = [e for e in additional_exams if e not in exams]
                if additional:
                    exams.append(np.random.choice(additional))

            interests_count = np.random.randint(2, min(4, len(faculty["keywords"])))
            interests_keywords = np.random.choice(faculty["keywords"], size=interests_count, replace=False)
            interests = ", ".join(interests_keywords)

            interest_variations = {
                "–ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ": ["–ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ", "–∫–æ–¥–∏–Ω–≥", "—Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∞ –ø—Ä–æ–≥—Ä–∞–º–º", "–Ω–∞–ø–∏—Å–∞–Ω–∏–µ –∫–æ–¥–∞"],
                "—ç–ª–µ–∫—Ç—Ä–æ–Ω–∏–∫–∞": ["—ç–ª–µ–∫—Ç—Ä–æ–Ω–∏–∫–∞", "—ç–ª–µ–∫—Ç—Ä–æ–Ω–Ω—ã–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞", "–º–∏–∫—Ä–æ—ç–ª–µ–∫—Ç—Ä–æ–Ω–∏–∫–∞"],
                "—É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ": ["—É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ", "–º–µ–Ω–µ–¥–∂–º–µ–Ω—Ç", "—Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ", "–∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∏—Ä–æ–≤–∞–Ω–∏–µ"],
                "–ø—Å–∏—Ö–æ–ª–æ–≥–∏—è": ["–ø—Å–∏—Ö–æ–ª–æ–≥–∏—è", "–∏–∑—É—á–µ–Ω–∏–µ –ø–æ–≤–µ–¥–µ–Ω–∏—è", "—Ä–∞–±–æ—Ç–∞ —Å –ª—é–¥—å–º–∏"],
                "—Ä–∞–¥–∏–æ—Ç–µ—Ö–Ω–∏–∫–∞": ["—Ä–∞–¥–∏–æ—Ç–µ—Ö–Ω–∏–∫–∞", "—Ä–∞–¥–∏–æ—Å–≤—è–∑—å", "–±–µ—Å–ø—Ä–æ–≤–æ–¥–Ω—ã–µ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏"]
            }
            
            for keyword in interests_keywords:
                if keyword in interest_variations and np.random.random() < 0.3:
                    interests = interests.replace(keyword, np.random.choice(interest_variations[keyword]))

            other_faculties = [f for f in faculty_codes if f != faculty_code]
            other_faculty = np.random.choice(other_faculties)
            not_interests_keywords = np.random.choice(TUSUR_FACULTIES[other_faculty]["keywords"], 
                                                     size=min(2, len(TUSUR_FACULTIES[other_faculty]["keywords"])), 
                                                     replace=False)
            not_interests = ", ".join(not_interests_keywords)
            
            data.append({
                "liked_subjects": liked,
                "disliked_subjects": disliked,
                "exams": exams,
                "interests": self.lemmatize_text(interests),
                "not_interests": self.lemmatize_text(not_interests),
                "faculty": faculty_code
            })
        
        print(f"–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞: {len(data)} –æ–±—Ä–∞–∑—Ü–æ–≤")
        return pd.DataFrame(data)
    
    def encode_text_features(self, text, keywords):
        if not text:
            return [0] * len(keywords)
        
        lemmatized = self.lemmatize_text(text)
        encoded = []
        
        for keyword in keywords:
            score = 0
            if keyword in lemmatized:
                score = 1
            elif any(word in lemmatized for word in keyword.split()):
                score = 0.5
            encoded.append(score)
        
        return encoded
    
    def prepare_features(self, df):
        print("–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤...")

        liked_encoded = self.mlb.fit_transform(df['liked_subjects'])
        disliked_encoded = self.mlb.transform(df['disliked_subjects'])
        exams_encoded = self.mlb.transform(df['exams'])
        
        print(f"–†–∞–∑–º–µ—Ä—ã –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –ø—Ä–µ–¥–º–µ—Ç–æ–≤: {liked_encoded.shape}")

        interests_encoded = np.array([
            self.encode_text_features(text, self.all_keywords) 
            for text in df['interests']
        ])
        not_interests_encoded = np.array([
            self.encode_text_features(text, self.all_keywords) 
            for text in df['not_interests']
        ])
        
        print(f"–†–∞–∑–º–µ—Ä—ã —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {interests_encoded.shape}")

        X = np.hstack([
            liked_encoded,      # 28 –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
            disliked_encoded,   # 28 –ø—Ä–∏–∑–Ω–∞–∫–æ–≤  
            exams_encoded,      # 28 –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
            interests_encoded,  # –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
            not_interests_encoded  # –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
        ])
        
        print(f"–ò—Ç–æ–≥–æ–≤–∞—è —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {X.shape}")
        return X
    
    def train_model(self, num_samples=2000):
        print("üß† –ù–∞—á–∏–Ω–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ ML –º–æ–¥–µ–ª–∏ –¥–ª—è –¢–£–°–£–†...")

        df = self.generate_training_data(num_samples)

        X = self.prepare_features(df)

        y = self.label_encoder.fit_transform(df['faculty'])
        print(f"–§–∞–∫—É–ª—å—Ç–µ—Ç—ã: {list(self.label_encoder.classes_)}")

        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)
        print(f"–û–±—É—á–∞—é—â–∞—è –≤—ã–±–æ—Ä–∫–∞: {X_train.shape}, –¢–µ—Å—Ç–æ–≤–∞—è –≤—ã–±–æ—Ä–∫–∞: {X_test.shape}")

        print("–°–æ–∑–¥–∞–Ω–∏–µ –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏...")
        self.model = Sequential([
            Dense(128, activation='relu', input_shape=(X.shape[1],)),
            Dropout(0.3),
            Dense(64, activation='relu'),
            Dropout(0.2),
            Dense(32, activation='relu'),
            Dropout(0.1),
            Dense(len(TUSUR_FACULTIES), activation='softmax')
        ])

        self.model.compile(
            optimizer=Adam(learning_rate=0.001),
            loss='sparse_categorical_crossentropy',
            metrics=['accuracy']
        )
        
        print("–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –º–æ–¥–µ–ª–∏:")
        self.model.summary()

        print("–ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è...")
        history = self.model.fit(
            X_train, y_train,
            validation_data=(X_test, y_test),
            epochs=100,
            batch_size=32,
            verbose=1
        )

        test_loss, test_accuracy = self.model.evaluate(X_test, y_test, verbose=0)
        print(f"\n‚úÖ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
        print(f"–¢–æ—á–Ω–æ—Å—Ç—å –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–µ: {test_accuracy:.3f}")
        
        self.is_trained = True
        return history
    
    def predict_faculty(self, user_data):
        if not self.is_trained:
            print("‚ö†Ô∏è –ú–æ–¥–µ–ª—å –Ω–µ –æ–±—É—á–µ–Ω–∞! –ó–∞–ø—É—Å–∫–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ...")
            self.train_model()
        
        try:
            liked_enc = self.mlb.transform([user_data.get('liked_subjects', [])])
            disliked_enc = self.mlb.transform([user_data.get('disliked_subjects', [])])
            exams_enc = self.mlb.transform([user_data.get('exams', [])])
            
            interests_enc = np.array([
                self.encode_text_features(user_data.get('interests', ''), self.all_keywords)
            ])
            not_interests_enc = np.array([
                self.encode_text_features(user_data.get('not_interests', ''), self.all_keywords)
            ])

            X_input = np.hstack([
                liked_enc,
                disliked_enc,
                exams_enc,
                interests_enc,
                not_interests_enc
            ])

            probabilities = self.model.predict(X_input, verbose=0)[0]
            faculty_index = np.argmax(probabilities)
            faculty_code = self.label_encoder.inverse_transform([faculty_index])[0]
            confidence = probabilities[faculty_index]
            
            faculty_info = TUSUR_FACULTIES[faculty_code]

            explanation = self._generate_explanation(user_data, faculty_code, confidence)
            
            return {
                'code': faculty_code,
                'name': faculty_info['name'],
                'reason': explanation,
                'directions': "‚Ä¢ " + "\n‚Ä¢ ".join(faculty_info['keywords'][:4]),
                'confidence': float(confidence)
            }
            
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è: {e}")
            return {
                'code': '–§–ò–¢',
                'name': TUSUR_FACULTIES['–§–ò–¢']['name'],
                'reason': '–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –≤—ã–±–æ—Ä –¥–ª—è —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–π (fallback)',
                'directions': '‚Ä¢ –ü—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ\n‚Ä¢ –ò–Ω–Ω–æ–≤–∞—Ü–∏–∏\n‚Ä¢ IT-—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏',
                'confidence': 0.5
            }
    
    def _generate_explanation(self, user_data, faculty_code, confidence):
        faculty = TUSUR_FACULTIES[faculty_code]
        liked_subjects = user_data.get('liked_subjects', [])
        interests = user_data.get('interests', '')
        
        explanations = []

        matching_subjects = [s for s in liked_subjects if s in faculty['liked_subjects']]
        if matching_subjects:
            explanations.append(f"–í–∞—à–∏ –ª—é–±–∏–º—ã–µ –ø—Ä–µ–¥–º–µ—Ç—ã ({', '.join(matching_subjects)}) —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—Ç –ø—Ä–æ—Ñ–∏–ª—é —Ñ–∞–∫—É–ª—å—Ç–µ—Ç–∞")

        matching_keywords = [k for k in faculty['keywords'] if k.lower() in interests.lower()]
        if matching_keywords:
            explanations.append(f"–í–∞—à–∏ –∏–Ω—Ç–µ—Ä–µ—Å—ã —Å–æ–≤–ø–∞–¥–∞—é—Ç —Å –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è–º–∏ —Ñ–∞–∫—É–ª—å—Ç–µ—Ç–∞")

        if confidence > 0.8:
            explanations.append(f"–í—ã—Å–æ–∫–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏ ({confidence:.1%})")
        elif confidence > 0.6:
            explanations.append(f"–•–æ—Ä–æ—à–µ–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ ({confidence:.1%})")
        else:
            explanations.append(f"–í–æ–∑–º–æ–∂–Ω—ã–π –≤–∞—Ä–∏–∞–Ω—Ç –¥–ª—è —Ä–∞—Å—Å–º–æ—Ç—Ä–µ–Ω–∏—è ({confidence:.1%})")
        
        return ". ".join(explanations) if explanations else f"–†–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞–Ω —Å–∏—Å—Ç–µ–º–æ–π —Å —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å—é {confidence:.1%}"
    
    def save_model(self):
        if self.model and self.is_trained:
            model_file = self.model_path / "tusur_model.keras"
            data_file = self.model_path / "model_data.pkl"

            self.model.save(model_file)

            with open(data_file, 'wb') as f:
                pickle.dump({
                    'mlb': self.mlb,
                    'label_encoder': self.label_encoder, 
                    'all_keywords': self.all_keywords,
                    'is_trained': self.is_trained
                }, f)
            
            print(f"‚úÖ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ {model_file}")
            return True
        return False
    
    def load_model(self):
        model_file = self.model_path / "tusur_model.keras"
        data_file = self.model_path / "model_data.pkl"
        
        try:
            if model_file.exists() and data_file.exists():
                from tensorflow.keras.models import load_model

                self.model = load_model(model_file)

                with open(data_file, 'rb') as f:
                    data = pickle.load(f)
                    self.mlb = data['mlb']
                    self.label_encoder = data['label_encoder']
                    self.all_keywords = data['all_keywords']
                    self.is_trained = data.get('is_trained', True)
                
                print(f"‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∏–∑ {model_file}")
                return True
            else:
                print("‚ö†Ô∏è –°–æ—Ö—Ä–∞–Ω–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞, –±—É–¥–µ—Ç —Å–æ–∑–¥–∞–Ω–∞ –Ω–æ–≤–∞—è")
                return False
                
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {e}")
            return False

faculty_predictor = TusurFacultyPredictor()

def initialize_model():
    if not faculty_predictor.load_model():
        print("üîÑ –ü–µ—Ä–≤—ã–π –∑–∞–ø—É—Å–∫ - –æ–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å...")
        faculty_predictor.train_model()
        faculty_predictor.save_model()

def test_model():
    test_cases = [
        {
            'name': 'IT-–æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∞–±–∏—Ç—É—Ä–∏–µ–Ω—Ç',
            'data': {
                'liked_subjects': ['–∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–∫–∞', '–º–∞—Ç–µ–º–∞—Ç–∏–∫–∞', '—Ñ–∏–∑–∏–∫–∞'],
                'disliked_subjects': ['–ª–∏—Ç–µ—Ä–∞—Ç—É—Ä–∞', '–∏—Å—Ç–æ—Ä–∏—è'],
                'exams': ['–∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–∫–∞', '–º–∞—Ç–µ–º–∞—Ç–∏–∫–∞'],
                'interests': '–ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ, –∞–ª–≥–æ—Ä–∏—Ç–º—ã, —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∞',
                'not_interests': '–ª–∏—Ç–µ—Ä–∞—Ç—É—Ä–∞, –ø–æ—ç–∑–∏—è'
            }
        },
        {
            'name': '–¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–π –ø—Ä–æ—Ñ–∏–ª—å',
            'data': {
                'liked_subjects': ['—Ñ–∏–∑–∏–∫–∞', '–º–∞—Ç–µ–º–∞—Ç–∏–∫–∞', '—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—è'],
                'disliked_subjects': ['–æ–±—â–µ—Å—Ç–≤–æ–∑–Ω–∞–Ω–∏–µ', '–º—Ö–∫'],
                'exams': ['—Ñ–∏–∑–∏–∫–∞', '–º–∞—Ç–µ–º–∞—Ç–∏–∫–∞'],
                'interests': '—ç–ª–µ–∫—Ç—Ä–æ–Ω–∏–∫–∞, —Å—Ö–µ–º—ã, –∞–≤—Ç–æ–º–∞—Ç–∏–∫–∞',
                'not_interests': '–≥—É–º–∞–Ω–∏—Ç–∞—Ä–Ω—ã–µ –Ω–∞—É–∫–∏'
            }
        },
        {
            'name': '–ì—É–º–∞–Ω–∏—Ç–∞—Ä–Ω—ã–π –ø—Ä–æ—Ñ–∏–ª—å',
            'data': {
                'liked_subjects': ['–ª–∏—Ç–µ—Ä–∞—Ç—É—Ä–∞', '–∏—Å—Ç–æ—Ä–∏—è', '–æ–±—â–µ—Å—Ç–≤–æ–∑–Ω–∞–Ω–∏–µ'],
                'disliked_subjects': ['—Ñ–∏–∑–∏–∫–∞', '–º–∞—Ç–µ–º–∞—Ç–∏–∫–∞'],
                'exams': ['–ª–∏—Ç–µ—Ä–∞—Ç—É—Ä–∞', '–æ–±—â–µ—Å—Ç–≤–æ–∑–Ω–∞–Ω–∏–µ'],
                'interests': '–ø—Å–∏—Ö–æ–ª–æ–≥–∏—è, –æ–±—â–µ–Ω–∏–µ, –∫—É–ª—å—Ç—É—Ä–∞',
                'not_interests': '–ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ, —Ç–µ—Ö–Ω–∏–∫–∞'
            }
        }
    ]
    
    print("\nüß™ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏:")
    for test_case in test_cases:
        print(f"\n--- {test_case['name']} ---")
        result = faculty_predictor.predict_faculty(test_case['data'])
        print(f"–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è: {result['name']}")
        print(f"–û–±–æ—Å–Ω–æ–≤–∞–Ω–∏–µ: {result['reason']}")
        print(f"–£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {result['confidence']:.2%}")

if __name__ == "__main__":
    initialize_model()
    test_model()